{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Get the list of master's degree courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import glob\n",
    "from nltk.stem import *\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from defs import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Via the HTTP GET request we retrieve the content of the url of our interest, in our case the page that contains all the masters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.findamasters.com/masters-degrees/msc-degrees/'\n",
    "result = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract all the links of the master's degree of the first 400 pages we used a function, which is located in the *defs.py* file; then we store all the links in the file *masters_urls.txt*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to extract all the masters we have to do a for loop for the first 400 pages:\n",
    "\n",
    "num_pages = 400\n",
    "pref = 'https://www.findamasters.com'\n",
    "test_lst_all = []\n",
    "\n",
    "for i in range(1, num_pages + 1):\n",
    "    test_lst = extract_masters(pref + '/masters-degrees/msc-degrees/?PG=' + str(i))\n",
    "    test_lst_all.extend(test_lst)\n",
    "# creating the txt file of the fisrt 400 pages of ms\n",
    "with open('masters_urls.txt', 'w') as f:\n",
    "    for item in test_lst_all:\n",
    "        f.write(item[0] + '\\n')\n",
    "# file created "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Crawl master's degree pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to save each HTML page of each course in a different folder, one folder for each page it is in; so we'll obtain 400 folders, each of them will contain 15 HTML files. The urls are taken from the *masters_urls.txt* previusly created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings for the User-Agent to simulate a browser\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36'}\n",
    "folder_name = \"html_pages\" # create the folder that will contain the html pages\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "for i in range(1, 401):\n",
    "    # create a folder for each page, from 1 to 400\n",
    "    name = 'HTML page ' + str(i)\n",
    "    path_folder = os.path.join(folder_name, name)\n",
    "    if not os.path.exists(path_folder):\n",
    "        os.makedirs(path_folder)\n",
    "\n",
    "# open the file containing the urls\n",
    "with open('masters_urls.txt', 'r') as file:\n",
    "    for index, url in enumerate(file):\n",
    "        url = url.strip()\n",
    "        page = (index // 15 ) + 1\n",
    "        try:\n",
    "            # the complete url\n",
    "            full_url = \"https://www.findamasters.com\" + url.strip()\n",
    "\n",
    "            # add a delay of 1 to 5 seconds between the requests\n",
    "            time.sleep(1 + random.uniform(0, 4))\n",
    "            # request to obtain the content of the url\n",
    "            response = requests.get(full_url, headers=headers)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                # Parsing dell'HTML con BeautifulSoup\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                \n",
    "                # save the html of the course in a separate file in the folder of the page it belongs to \n",
    "                file_path = os.path.join(f\"{folder_name}\\HTML page {page}\", f\"course {index+1}.html\")\n",
    "                with open(file_path, \"w\", encoding=\"utf-8\") as html_file:\n",
    "                    html_file.write(str(soup))\n",
    "\n",
    "            else:\n",
    "                print(f\"Errore nel recuperare la pagina del corso: {full_url}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Errore durante il recupero e salvataggio della pagina {full_url}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Parse download pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through the *extcat_msc_page* function (located in the *defs.py*) we parse all the HTML we retrieved before and collect all the information for each master."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Directory where there are the HTML repositories\n",
    "html_folder = \"\\html-pages\"\n",
    "my_path = \"D:\\Primo Semestre\\ADM\\HW3\"\n",
    "#List to contain all the information\n",
    "all_master_info = []\n",
    "\n",
    "all_url = []\n",
    "\n",
    "#Iterating whitin the repositories HTML of every page\n",
    "for page_folder in os.listdir(my_path + html_folder):\n",
    "    page_path = os.path.join(html_folder, page_folder)\n",
    "    file_absolute_path = os.path.join(my_path + page_path)\n",
    "\n",
    "    if os.path.isdir(file_absolute_path):\n",
    "        #Iterating in the files HTML of every repository\n",
    "        for file in os.listdir(file_absolute_path):\n",
    "            if file.endswith(\".html\"):\n",
    "                file_path = os.path.join(file_absolute_path, file)\n",
    "                print('FILE PATH: ', file_path)\n",
    "                #Applying the function extract_msc_page to every file HTML\n",
    "                master_info = extract_msc_page(file_path) \n",
    "                all_master_info.extend(master_info)\n",
    "              \n",
    "print(all_master_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we're storing those information in a tsv file, one for each master. All the files are stored in a folder called 'Tsv files'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the tsv file for each master\n",
    "\n",
    "folder_name = \"Tsv files\"\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "folder_path = \"D:\\Primo Semestre\\ADM\\HW3\\Tsv files\"\n",
    "\n",
    "for i in range(0,len(all_master_info)):\n",
    "    output_file = f\"{folder_path}\\course_{i+1}.tsv\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as tsvfile:\n",
    "    # Extract field names from the dictionaries in 'all_master_info'\n",
    "        fieldnames = all_master_info[i].keys()\n",
    "        tsvfile.write('\\t'.join(fieldnames) + '\\n')                  # write the header\n",
    "        row = '\\t'.join(str(all_master_info[i].get(field, '')) for field in fieldnames)\n",
    "        tsvfile.write(row + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create our dataframe, reading the data on all the tsv files we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frames = []\n",
    "file_name_list = glob.glob(\"Tsv files\\course_*.tsv\")    # take all the course_i.tsv files\n",
    "for file in file_name_list:\n",
    "    dF_tsv = pd.read_csv(file, sep='\\t', header=0)      # create the data frame from the tsv file\n",
    "    data_frames.append(dF_tsv)\n",
    "# Concatenate all DataFrames in the list into a single DataFrame\n",
    "dataset = pd.concat(data_frames, ignore_index=True)     # creating the whole dataframe from each tsv file\n",
    "\n",
    "dataset = dataset[dataset.description != ''] # do not ocnsider all the rows that have an empty description\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**da eliminare poi questo**  perchè per semplicitò apriamo il dataset da un file json, ma in realtà dovremmo aprirlo dai file tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the json file for the dataset (but it will be the tsv file)\n",
    "path = r\"D:\\Primo Semestre\\ADM\\HW3\\university_dataset.json\"\n",
    "dataset= pd.read_json(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.0.0 Preprocessing the text\\\n",
    "We created 3 functions, that are present in the *defs.py*, to perform the stemming, remove the stopwords and punctuation from the *description* field of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'clean_description_s' from 'defs' (c:\\Users\\anton\\Documents\\GitHub\\ADM---HW3\\defs.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\anton\\Documents\\GitHub\\ADM---HW3\\main.ipynb Cell 24\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/anton/Documents/GitHub/ADM---HW3/main.ipynb#Y120sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdefs\u001b[39;00m \u001b[39mimport\u001b[39;00m clean_description_s\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'clean_description_s' from 'defs' (c:\\Users\\anton\\Documents\\GitHub\\ADM---HW3\\defs.py)"
     ]
    }
   ],
   "source": [
    "from defs import clean_description_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clean_description_s' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\anton\\Documents\\GitHub\\ADM---HW3\\main.ipynb Cell 24\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/anton/Documents/GitHub/ADM---HW3/main.ipynb#X34sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m dataset[\u001b[39m'\u001b[39m\u001b[39mdescr_stem\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m dataset[\u001b[39m'\u001b[39m\u001b[39mdescription\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(stem_description)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/anton/Documents/GitHub/ADM---HW3/main.ipynb#X34sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# 2. removing stopwords\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/anton/Documents/GitHub/ADM---HW3/main.ipynb#X34sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m dataset[\u001b[39m'\u001b[39m\u001b[39mdescription_clean\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m dataset[\u001b[39m'\u001b[39m\u001b[39mdescription\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(clean_description_s)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/anton/Documents/GitHub/ADM---HW3/main.ipynb#X34sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# 3. removing punctuation\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/anton/Documents/GitHub/ADM---HW3/main.ipynb#X34sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m dataset[\u001b[39m'\u001b[39m\u001b[39mdescription_clean\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m dataset[\u001b[39m'\u001b[39m\u001b[39mdescription\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(clean_description_p)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'clean_description_s' is not defined"
     ]
    }
   ],
   "source": [
    "# 1. stemming\n",
    "dataset['descr_stem'] = dataset['description'].apply(stem_description)\n",
    "\n",
    "# 2. removing stopwords\n",
    "dataset['description_clean'] = dataset['description'].apply(clean_description_s)\n",
    "\n",
    "# 3. removing punctuation\n",
    "dataset['description_clean'] = dataset['description'].apply(clean_description_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.0.1 preprocess the *fees* column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Conjuctive query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.1 Create the index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created the vocabulary assigning an unique ID to each word encoutered in the description field of the dataset, then created a csv file out of it, to store the information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the vocabulary \n",
    "vocabulary = Counter(reduce(lambda x,y : x+y, dataset.description_clean)).keys()\n",
    "\n",
    "# assign an unique ID to each word of the vocabulary using a pandas dataframe\n",
    "terms = pd.DataFrame(data=list(vocabulary), columns=['term'])\n",
    "\n",
    "terms\n",
    "# creating a csv file for the vocabulary with index of each term\n",
    "terms.to_csv('vocabulary.csv', index_label='term_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the inverted index as a new column of the dataframe *terms* and store it in a txt file, called *Inverted Index.txt*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms['reverse'] = terms.term.apply(lambda item: list(dataset.loc[dataset.description_clean.apply(lambda row: item in row)].index))\n",
    "terms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now transform the inverted index in a dictionary in this form\\\n",
    "<code> {\n",
    "term_id_1:[document_1, document_2, document_4],\n",
    "term_id_2:[document_1, document_3, document_5, document_6],\n",
    "    ...}\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "InvertedIndex = terms['reverse'].to_dict()\n",
    "print(InvertedIndex) # the dictionary of the inverted index \n",
    "\n",
    "# store the inverted index in a txt file\n",
    "with open('Inverted Index.txt', 'w') as file:\n",
    "\n",
    "    for key, value in InvertedIndex.items():\n",
    "        file.write(f'{key}: {value}\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read back the inverted index from the file.\n",
    "\n",
    "file = open(\"Inverted Index.txt\", \"r\")\n",
    "\n",
    "inv_indx = dict()\n",
    "txt = file.read().split(\"\\n\")\n",
    "\n",
    "for i in range(len(txt)-1):\n",
    "    line = txt[i].replace(\":\", \"\").replace(\"[\", \"\").replace(\"]\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\",\", \"\").split(\" \")\n",
    "    inv_indx[int(line[0])] = []\n",
    "    for j in range(1, len(line)):\n",
    "            inv_indx[int(line[0])].append(int(line[j]))\n",
    "            \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.2 Execute the query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a function called *query_preprocess* that preprocesses the query just like we did in the preprocess of the description field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query = str(input())\n",
    "query = 'advanced knoledge'\n",
    "# formatting the query \n",
    "query = query_preprocess(query) \n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we're going to do now to implement our Search Engine is:\n",
    "- Find all the words of the query in the vocabulary and exctract each *term_id* of each word of the query.\n",
    "- Find all the documents related to each *term_id* in the Inverted Index.\n",
    "- Do the intersection of the lists of documents found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = pd.read_csv('vocabulary.csv') # read the vocabulary file into a dataframe\n",
    "vocabulary = pd.DataFrame(vocabulary)\n",
    "\n",
    "file = open(\"Inverted Index.txt\", \"r\") # read the inverted index from the file.\n",
    "\n",
    "inv_indx = dict()\n",
    "txt = file.read().split(\"\\n\")\n",
    "\n",
    "for i in range(len(txt)-1):\n",
    "    line = txt[i].replace(\":\", \"\").replace(\"[\", \"\").replace(\"]\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\",\", \"\").split(\" \")\n",
    "    inv_indx[int(line[0])] = []\n",
    "    for j in range(1, len(line)):\n",
    "            inv_indx[int(line[0])].append(int(line[j]))    \n",
    "file.close()\n",
    "\n",
    "# find the words of the query in the vocabulary\n",
    "for w in query:\n",
    "    term_ids = [vocabulary[vocabulary['term'] == w]['term_id'].values for w in query if w in vocabulary.term.values]\n",
    "\n",
    "term_ids = [term_ids[x][0] for x in range(len(term_ids))] # exctract only the integers of the ids\n",
    "\n",
    "# find the documents \n",
    "docs = [inv_indx[i] for i in term_ids]\n",
    "\n",
    "# intersecting the two sets of documents we found out contain all the word of the query\n",
    "intersection = list(set(docs[0]).intersection(*docs[1:]))\n",
    "print('All the documents of the result:',intersection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can show the results of the query after it passed into the search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information_needed = ['courseName','universityName','description', 'url']\n",
    "dataset.loc[intersection,information_needed] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Conjunctive query & Ranking score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.1 Inverted Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to implement a ranking system, computing the *TF-IDF* for each word in each document, and then calculating the *cosine similarity* between the query vector and each one of the vectors corresponding to the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf \n",
    "# use the library scikit-learn: tfidf implementation VECTORIZED \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert a collection of raw documents to a matrix of TF-IDF features\n",
    "\n",
    "tfidf = TfidfVectorizer(input='content', lowercase=False, tokenizer=lambda text: text)\n",
    "results = tfidf.fit_transform(dataset.description_clean) # fit data to train our model (but in our case is the same dataset)\n",
    "results_dense = results.todense() # results are sparse documents that i want to convert into a dense one\n",
    "\n",
    "# putting all into a dataframe where the index of the dataframe is each document id\n",
    "tfidf_data = pd.DataFrame(results_dense.tolist(), index=dataset.index, columns=tfidf.get_feature_names_out()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating our second inverted index in the form:\\\n",
    "<code>{\n",
    "term_id_1:[(document1, tfIdf_{term,document1}), (document2, tfIdf_{term,document2}), (document4, tfIdf_{term,document4}), ...],\n",
    "term_id_2:[(document1, tfIdf_{term,document1}), (document3, tfIdf_{term,document3}), (document5, tfIdf_{term,document5}), (document6, tfIdf_{term,document6}), ...],\n",
    "...}\n",
    "</code>\n",
    "And then storing it in a txt file called *Extented Inverted Index.txt*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the second inverted index\n",
    "from collections import defaultdict\n",
    "extended_inverted_index = defaultdict(list)\n",
    "\n",
    "# Iterate through each term in the inverted index\n",
    "for term_id, doc_indices in inv_indx.items():\n",
    "    # Iterate through each document index for the current term\n",
    "    \n",
    "    for doc_index in doc_indices:\n",
    "        # Get the TF-IDF scores for the current document \n",
    "        word = vocabulary[vocabulary['term_id'] == term_id]['term'].values\n",
    "     \n",
    "        if  word[0] in tfidf_data.columns: # check if the word is in the tfidf\n",
    "                tfidf_scores = tfidf_data.loc[doc_index,word[0]]\n",
    "        else:\n",
    "                continue\n",
    "\n",
    "        # Append a tuple of (document_index, TF-IDF scores) to the term's list in the extended inverted index\n",
    "        extended_inverted_index[term_id].append((doc_index, tfidf_scores))\n",
    "\n",
    "# Convert the extended inverted index defaultdict to a regular dictionary\n",
    "extended_inverted_index = dict(extended_inverted_index)\n",
    "print(extended_inverted_index)\n",
    "\n",
    "# save the extended inverted dictionary in a txt file as before\n",
    "with open('Extended Inverted Index.txt', 'w') as file:\n",
    "\n",
    "    for key, value in extended_inverted_index.items():\n",
    "        file.write(f'{key}: {value}\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the inverted index from the file.\n",
    "\n",
    "file = open(\"Extended Inverted Index.txt\", \"r\")\n",
    "\n",
    "ext_inv_indx = dict()\n",
    "txt = file.read().split(\"\\n\")\n",
    "\n",
    "for i in range(len(txt)-1):\n",
    "    line = txt[i].replace(\":\", \"\").replace(\"[\", \"\").replace(\"]\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\",\", \"\").split(\" \")\n",
    "    ext_inv_indx[int(line[0])] = []\n",
    "    for j in range(1, len(line)):\n",
    "        if j%2 == 1:\n",
    "            ext_inv_indx[int(line[0])].append((int(line[j]), float(line[j+1])))\n",
    "            \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.2 Execute the query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created the query vector, putting a 1 if the word corresponding to the position is present in the query, 0 if is not.\\\n",
    "The vector for each description is each row of the dataframe tfidf, so no need to compute it for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a vector for the query\n",
    "query_vec = np.zeros(vocabulary.shape[0]) # inizialize the vector\n",
    "for word in query:\n",
    "    if word in tfidf_data.columns:\n",
    "        term_id = vocabulary[vocabulary['term'] == word]['term_id']\n",
    "        query_vec[term_id] = 1.0\n",
    "\n",
    "# the documents matrix with all the tfidf is the dataframe tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need the cosine similarity function that we wrote in the *defs.py*, which simply exploits the definition of the cosine similaruty between two vectors that create the $\\phi$ angle:\n",
    "\n",
    "$cos(\\phi) = \\frac{\\vec{q} \\cdot \\vec{d}}{|{\\vec{q}}| \\cdot |{\\vec{d}}|}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using a list sto store the similarity scores, we can use a heap structure, to make the sorting more efficient from a computational point of view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heap = []\n",
    "scores_dictionary = {}\n",
    "\n",
    "# For every document\n",
    "for doc_index in range(tfidf_data.shape[0]):\n",
    "    doc_arr = tfidf_data.loc[doc_index, :].values\n",
    "    # Compute the angle between the doc and the query vector\n",
    "    cos_sim = a_cosine_similarity(query_vec, doc_arr)\n",
    "\n",
    "    # Put the result in the dictionary\n",
    "    scores_dictionary[doc_index] = cos_sim\n",
    "\n",
    "    # Update the heap\n",
    "    heapq.heappush(heap, (cos_sim, doc_index))  # Store both score and document index in the heap\n",
    "print(scores_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.2 Execute the query with k = 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 20\n",
    "top_k = heapq.nlargest(k, heap)\n",
    "print(top_k)\n",
    "\n",
    "top_doc_k = []\n",
    "\n",
    "#fill the list of top_k_doc\n",
    "for score, doc in top_k:\n",
    "    top_doc_k.append(doc)\n",
    "print(top_doc_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the column 'similarity score' to the dataset\n",
    "rinformation_needed = ['courseName','universityName','description', 'url']\n",
    "results = dataset.loc[top_doc_k , information_needed] \n",
    "results['similarity'] = [round(s[0],3) for s in top_k]\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
